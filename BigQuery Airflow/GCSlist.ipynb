{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 999\n",
    "pd.options.display.max_columns = 999\n",
    "#pip install google-cloud-storage\n",
    "from google.cloud import storage\n",
    "\n",
    "#pip install -U google-api-python-client\n",
    "from googleapiclient import discovery\n",
    "\n",
    "#pip install -U oauth2client\n",
    "from oauth2client.client import GoogleCredentials\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "#pip install pandas-gbq -U\n",
    "from google.cloud import bigquery\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "st = time.time()\n",
    "\n",
    "#Setup Application Default Credentials -- https://cloud.google.com/docs/authentication/client-libraries#python\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = os.getcwd() + \"\\\\application_default_credentials.json\"\n",
    "credentials = GoogleCredentials.get_application_default()\n",
    "\n",
    "service = discovery.build('cloudresourcemanager', 'v1', credentials=credentials)\n",
    " \n",
    "request = service.projects().list()\n",
    "project_list = []\n",
    "\n",
    "while request is not None:\n",
    "    response = request.execute()\n",
    "    for project in response.get('projects', []):\n",
    "        #print('{:<20} {:<22} {:<21}'.format(project['projectId'], project['name'], project['projectNumber']))\n",
    "        project_list.append(pd.json_normalize(project,sep='_'))\n",
    "    request = service.projects().list_next(previous_request=request, previous_response=response)\n",
    "\n",
    "project_df = pd.concat(project_list)\n",
    "project_df.rename(columns={\"labels_support-group\": \"labels_support_group\"},inplace=True)\n",
    "project_df['insert_datetime'] = pd.Timestamp.today()\n",
    "project_df = project_df.reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yash Gupta\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\auth\\_default.py:81: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. We recommend you rerun `gcloud auth application-default login` and make sure a quota project is added. Or you can use service accounts instead. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "bucket_list = []\n",
    "item_list = []\n",
    "storage_client = storage.Client(project='gcp-wow-rwds-ai-bi-prod')\n",
    "buckets = storage_client.list_buckets()\n",
    "for bucket in buckets:\n",
    "    bucket_list.append(pd.json_normalize(dict({'bucket_id':bucket.id,\n",
    "                        'bucket_name':bucket.name,\n",
    "                        'bucket_location':bucket.location,\n",
    "                        'bucket_location_type':bucket.location_type,\n",
    "                        'bucket_storage_class':bucket.storage_class,\n",
    "                        'bucket_owner':bucket.owner,\n",
    "                        'bucket_user_project':bucket.user_project,\n",
    "                        'bucket_project_number':bucket.project_number,\n",
    "                        'bucket_label':bucket.labels                     \n",
    "                       }),sep='_'))\n",
    "    blobs = bucket.list_blobs()\n",
    "    for item in blobs:\n",
    "        item_list.append(pd.json_normalize(dict({'bucket_id':bucket.id,\n",
    "                        'item_name':item.name,\n",
    "                        'item_size':item.size,\n",
    "                        'item_content_type':item.content_type,\n",
    "                        'item_storage_class':item.storage_class,\n",
    "                        'item_owner':item.owner,\n",
    "                        'item_time_created':item.time_created,\n",
    "                        'item_updated':item.updated                    \n",
    "                       }),sep='_'))\n",
    "        \n",
    "bucket_df = pd.concat(bucket_list)\n",
    "item_df = pd.concat(item_list)\n",
    "bucket_df.rename(columns={\"bucket_label_goog-composer-location\": \"bucket_label_goog_composer_location\",\n",
    "                         \"bucket_label_goog-composer-environment\":\"bucket_label_goog_composer_environment\",\n",
    "                          \"bucket_label_goog-composer-version\":\"bucket_label_goog_composer_version\"\n",
    "                         } ,inplace=True)\n",
    "\n",
    "bucket_df['insert_datetime'] = pd.Timestamp.today()\n",
    "item_df['insert_datetime'] = pd.Timestamp.today()\n",
    "bucket_df = bucket_df.reset_index(drop=True)\n",
    "item_df = item_df.reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yash Gupta\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\auth\\_default.py:81: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. We recommend you rerun `gcloud auth application-default login` and make sure a quota project is added. Or you can use service accounts instead. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 241 rows and 21 columns to gcp-wow-rwds-ai-bi-prod.airflow_logs.gcp_project_list\n"
     ]
    }
   ],
   "source": [
    "\n",
    "client = bigquery.Client(project='gcp-wow-rwds-ai-bi-prod')\n",
    "\n",
    "#Load Project Table\n",
    "project_table_id = \"gcp-wow-rwds-ai-bi-prod.airflow_logs.gcp_project_list\"\n",
    "job_config_project = bigquery.LoadJobConfig(\n",
    "\n",
    "    schema=[\n",
    "        bigquery.SchemaField(\"projectNumber\", bigquery.enums.SqlTypeNames.STRING),#ss\n",
    "        bigquery.SchemaField(\"projectId\", bigquery.enums.SqlTypeNames.STRING),\n",
    "        bigquery.SchemaField(\"lifecycleState\", bigquery.enums.SqlTypeNames.STRING),\n",
    "        bigquery.SchemaField(\"name\", bigquery.enums.SqlTypeNames.STRING),\n",
    "        bigquery.SchemaField(\"createTime\", bigquery.enums.SqlTypeNames.STRING),\n",
    "        bigquery.SchemaField(\"labels_business_unit\", bigquery.enums.SqlTypeNames.STRING),\n",
    "        bigquery.SchemaField(\"labels_cost_centre\", bigquery.enums.SqlTypeNames.STRING),\n",
    "        bigquery.SchemaField(\"labels_env\", bigquery.enums.SqlTypeNames.STRING),\n",
    "        bigquery.SchemaField(\"labels_environment\", bigquery.enums.SqlTypeNames.STRING),\n",
    "        bigquery.SchemaField(\"labels_name\", bigquery.enums.SqlTypeNames.STRING),\n",
    "        bigquery.SchemaField(\"labels_owner\", bigquery.enums.SqlTypeNames.STRING),\n",
    "        bigquery.SchemaField(\"labels_project_code\", bigquery.enums.SqlTypeNames.STRING),\n",
    "        bigquery.SchemaField(\"labels_squad\", bigquery.enums.SqlTypeNames.STRING),\n",
    "        bigquery.SchemaField(\"labels_support_contact\", bigquery.enums.SqlTypeNames.STRING),\n",
    "        bigquery.SchemaField(\"parent_type\", bigquery.enums.SqlTypeNames.STRING),\n",
    "        bigquery.SchemaField(\"parent_id\", bigquery.enums.SqlTypeNames.STRING),\n",
    "        bigquery.SchemaField(\"parent_type\", bigquery.enums.SqlTypeNames.STRING),\n",
    "        bigquery.SchemaField(\"labels_product\", bigquery.enums.SqlTypeNames.STRING),\n",
    "        bigquery.SchemaField(\"labels_exemption_id1\", bigquery.enums.SqlTypeNames.STRING),\n",
    "        bigquery.SchemaField(\"labels_exemption_id2\", bigquery.enums.SqlTypeNames.STRING),\n",
    "        bigquery.SchemaField(\"labels_support_group\", bigquery.enums.SqlTypeNames.STRING),\n",
    "        bigquery.SchemaField(\"insert_datetime\", bigquery.enums.SqlTypeNames.DATETIME)\n",
    "    ],\n",
    "    #write_disposition=\"WRITE_TRUNCATE\",\n",
    " )\n",
    "\n",
    "job_project = client.load_table_from_dataframe(\n",
    "    project_df, project_table_id, job_config=job_config_project\n",
    ")  # Make an API request.\n",
    "job_project.result()  # Wait for the job to complete.\n",
    "\n",
    "project_table = client.get_table(project_table_id)  # Make an API request.\n",
    "print(\n",
    "    \"Loaded {} rows and {} columns to {}\".format(\n",
    "        project_table.num_rows, len(project_table.schema), project_table_id\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 63 rows and 13 columns to gcp-wow-rwds-ai-bi-prod.airflow_logs.gcs_bucket_details\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bucket_table_id = \"gcp-wow-rwds-ai-bi-prod.airflow_logs.gcs_bucket_details\"\n",
    "job_config_bucket = bigquery.LoadJobConfig(\n",
    "    #schema_update_options=['ALLOW_FIELD_ADDITION'],\n",
    "    autodetect = True,\n",
    "    schema=[\n",
    "        bigquery.SchemaField(\"bucket_id\", bigquery.enums.SqlTypeNames.STRING),\n",
    "        bigquery.SchemaField(\"bucket_name\", bigquery.enums.SqlTypeNames.STRING),\n",
    "        bigquery.SchemaField(\"bucket_location\", bigquery.enums.SqlTypeNames.STRING),\n",
    "        bigquery.SchemaField(\"bucket_storage_class\", bigquery.enums.SqlTypeNames.STRING),\n",
    "        bigquery.SchemaField(\"bucket_owner\", bigquery.enums.SqlTypeNames.STRING),\n",
    "        bigquery.SchemaField(\"bucket_user_project\", bigquery.enums.SqlTypeNames.STRING),\n",
    "        bigquery.SchemaField(\"bucket_project_number\", bigquery.enums.SqlTypeNames.INT64),\n",
    "        bigquery.SchemaField(\"bucket_label_goog_composer_environment\", bigquery.enums.SqlTypeNames.STRING),\n",
    "        bigquery.SchemaField(\"bucket_label_goog_composer_version\", bigquery.enums.SqlTypeNames.STRING),\n",
    "        bigquery.SchemaField(\"bucket_label_goog_composer_location\", bigquery.enums.SqlTypeNames.STRING),\n",
    "        bigquery.SchemaField(\"bucket_label_name\", bigquery.enums.SqlTypeNames.STRING),\n",
    "        bigquery.SchemaField(\"insert_datetime\", bigquery.enums.SqlTypeNames.DATETIME)\n",
    "    ],\n",
    "    #write_disposition=\"WRITE_TRUNCATE\",\n",
    " )\n",
    "\n",
    "job_bucket = client.load_table_from_dataframe(\n",
    "    bucket_df, bucket_table_id, job_config=job_config_bucket\n",
    ")  # Make an API request.\n",
    "job_bucket.result()  # Wait for the job to complete.\n",
    "\n",
    "bucket_table = client.get_table(bucket_table_id)  # Make an API request.\n",
    "print(\n",
    "    \"Loaded {} rows and {} columns to {}\".format(\n",
    "        bucket_table.num_rows, len(bucket_table.schema), bucket_table_id\n",
    "    )\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4136977 rows and 9 columns to gcp-wow-rwds-ai-bi-prod.airflow_logs.gcs_object_details\n"
     ]
    }
   ],
   "source": [
    "item_table_id = \"gcp-wow-rwds-ai-bi-prod.airflow_logs.gcs_object_details\"\n",
    "\n",
    "job_config_item = bigquery.LoadJobConfig(\n",
    "     autodetect = True,\n",
    "#     schema=[\n",
    "#         bigquery.SchemaField(\"bucket_id\", bigquery.enums.SqlTypeNames.STRING),\n",
    "#         bigquery.SchemaField(\"bucket_name\", bigquery.enums.SqlTypeNames.STRING),\n",
    "#         bigquery.SchemaField(\"bucket_location\", bigquery.enums.SqlTypeNames.STRING),\n",
    "#         bigquery.SchemaField(\"bucket_storage_class\", bigquery.enums.SqlTypeNames.STRING),\n",
    "#         bigquery.SchemaField(\"bucket_owner\", bigquery.enums.SqlTypeNames.STRING),\n",
    "#         bigquery.SchemaField(\"bucket_user_project\", bigquery.enums.SqlTypeNames.STRING),\n",
    "#         bigquery.SchemaField(\"bucket_project_number\", bigquery.enums.SqlTypeNames.INT64),\n",
    "#         bigquery.SchemaField(\"bucket_label_goog_composer_environment\", bigquery.enums.SqlTypeNames.STRING),\n",
    "#         bigquery.SchemaField(\"bucket_label_goog_composer_version\", bigquery.enums.SqlTypeNames.STRING),\n",
    "#         bigquery.SchemaField(\"bucket_label_goog_composer_location\", bigquery.enums.SqlTypeNames.STRING),\n",
    "#         bigquery.SchemaField(\"bucket_label_name\", bigquery.enums.SqlTypeNames.STRING),\n",
    "#         bigquery.SchemaField(\"insert_datetime\", bigquery.enums.SqlTypeNames.DATETIME)\n",
    "#     ],\n",
    "    #write_disposition=\"WRITE_TRUNCATE\",\n",
    " #   schema_update_options = 'ALLOW_FIELD_ADDITION'\n",
    " )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "job_item = client.load_table_from_dataframe(\n",
    "    item_df, item_table_id, job_config=job_config_item\n",
    ")  # Make an API request.\n",
    "job_item.result()  # Wait for the job to complete.\n",
    "\n",
    "item_table = client.get_table(item_table_id)  # Make an API request.\n",
    "print(\n",
    "    \"Loaded {} rows and {} columns to {}\".format(\n",
    "        item_table.num_rows, len(item_table.schema), item_table_id\n",
    "    )\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 1075.484159231186 seconds\n"
     ]
    }
   ],
   "source": [
    "et = time.time()\n",
    "elapsed_time = et - st\n",
    "print('Execution time:', elapsed_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'http://www.foxnews.com/' page is 285993 bytes\n",
      "'http://www.bbc.co.uk/' page is 318933 bytes\n",
      "'http://europe.wsj.com/' generated an exception: HTTP Error 403: Forbidden\n",
      "'http://www.cnn.com/' page is 1144964 bytes\n",
      "'http://some-made-up-domain.com/' generated an exception: HTTP Error 403: Forbidden\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "import urllib.request\n",
    "list_url = []\n",
    "URLS = ['http://www.foxnews.com/',\n",
    "        'http://www.cnn.com/',\n",
    "        'http://europe.wsj.com/',\n",
    "        'http://www.bbc.co.uk/',\n",
    "        'http://some-made-up-domain.com/']\n",
    "\n",
    "# Retrieve a single page and report the URL and contents\n",
    "def load_url(url, timeout):\n",
    "    with urllib.request.urlopen(url, timeout=timeout) as conn:\n",
    "        list_url.append(url)\n",
    "        return conn.read()\n",
    "\n",
    "# We can use a with statement to ensure threads are cleaned up promptly\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    # Start the load operations and mark each future with its URL\n",
    "    future_to_url = {executor.submit(load_url, url, 60): url for url in URLS}\n",
    "    for future in concurrent.futures.as_completed(future_to_url):\n",
    "        url = future_to_url[future]\n",
    "        try:\n",
    "            data = future.result()\n",
    "        except Exception as exc:\n",
    "            print('%r generated an exception: %s' % (url, exc))\n",
    "        else:\n",
    "            print('%r page is %d bytes' % (url, len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# st = time.time()\n",
    "# import concurrent.futures\n",
    "# import urllib.request\n",
    "# #get details for a single bucket\n",
    "# def f_item_detail(item):\n",
    "#     return (pd.json_normalize(dict({'bucket_id':bucket.id,\n",
    "#                     'item_name':item.name,\n",
    "#                     'item_size':item.size,\n",
    "#                     'item_content_type':item.content_type,\n",
    "#                     'item_storage_class':item.storage_class,\n",
    "#                     'item_owner':item.owner,\n",
    "#                     'item_time_created':item.time_created,\n",
    "#                     'item_updated':item.updated                    \n",
    "#                    }),sep='_'))\n",
    "\n",
    "\n",
    "# bucket_list = []\n",
    "# item_list = []\n",
    "# i=0\n",
    "# data =[]\n",
    "# storage_client = storage.Client(project='gcp-wow-rwds-ai-bi-prod')\n",
    "# buckets = storage_client.list_buckets()\n",
    "# # for bucket in buckets:\n",
    "# #     bucket_list.append(pd.json_normalize(dict({'bucket_id':bucket.id,\n",
    "# #                         'bucket_name':bucket.name,\n",
    "# #                         'bucket_location':bucket.location,\n",
    "# #                         'bucket_location_type':bucket.location_type,\n",
    "# #                         'bucket_storage_class':bucket.storage_class,\n",
    "# #                         'bucket_owner':bucket.owner,\n",
    "# #                         'bucket_user_project':bucket.user_project,\n",
    "# #                         'bucket_project_number':bucket.project_number,\n",
    "# #                         'bucket_label':bucket.labels                     \n",
    "# #                        }),sep='_'))\n",
    "#     # We can use a with statement to ensure threads are cleaned up promptly\n",
    "# for bucket in buckets:\n",
    "#     blobs = bucket.list_blobs()\n",
    "#     with concurrent.futures.ThreadPoolExecutor(max_workers=100) as executor:\n",
    "#         # Start the load operations and mark each future with its URL\n",
    "#         future_to_item = {executor.submit(f_item_detail, item): item for item in blobs}\n",
    "#         for future in concurrent.futures.as_completed(future_to_item):\n",
    "#             try:\n",
    "#                 #item_list.append(future.result())\n",
    "#                 data = future.result()\n",
    "#             except Exception as exc:\n",
    "#                 print('%r generated an exception: %s' % (item, exc))\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "# # # bucket_df = pd.concat(bucket_list)\n",
    "# # item_df = pd.concat(item_list)\n",
    "# # # bucket_df.rename(columns={\"bucket_label_goog-composer-location\": \"bucket_label_goog_composer_location\",\n",
    "# # #                          \"bucket_label_goog-composer-environment\":\"bucket_label_goog_composer_environment\",\n",
    "# # #                           \"bucket_label_goog-composer-version\":\"bucket_label_goog_composer_version\"\n",
    "# # #                          } ,inplace=True)\n",
    "\n",
    "# # # bucket_df['insert_datetime'] = pd.Timestamp.today()\n",
    "# # item_df['insert_datetime'] = pd.Timestamp.today()\n",
    "# # # bucket_df = bucket_df.reset_index(drop=True)\n",
    "# # item_df = item_df.reset_index(drop=True)\n",
    "# print(data)\n",
    "# et = time.time()\n",
    "# elapsed_time = et - st\n",
    "# print('Execution time:', elapsed_time, 'seconds')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
